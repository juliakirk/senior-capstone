{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['medianValue', 'restaurants_pp', 'public_transport_percent', 'walked_percent', 'black_percent', 'asian_percent', 'two_plus_percent', 'hispanic_percent'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m normalized_data \u001b[38;5;241m=\u001b[39m city_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedianValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedianRent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrime_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparks_pp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpop_density\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestaurants_pp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnightlife_pp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbars_pp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternational_airport\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeign_born_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_avg_high\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_avg_low\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_diff\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[0;32m---> 14\u001b[0m normalized_data[metrics] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(city_data[metrics])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['medianValue', 'restaurants_pp', 'public_transport_percent', 'walked_percent', 'black_percent', 'asian_percent', 'two_plus_percent', 'hispanic_percent'] not in index\""
     ]
    }
   ],
   "source": [
    "city_data = pd.read_csv(\"algorithmData.csv\")  # Replace with your dataset\n",
    "\n",
    "# Normalize relevant columns\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = city_data.copy()\n",
    "metrics = [\n",
    "    \"medianValue\", \"medianRent\", \"crime_rate\", \"parks_pp\", \"pop_density\",\n",
    "    \"restaurants_pp\", \"nightlife_pp\", \"bars_pp\", \"international_airport\",\n",
    "    \"amtrak\", \"car_percent\", \"public_transport_percent\", \"walked_percent\",\n",
    "    \"museums_pp\", \"activities_pp\", \"republican\", \"democrat\", \"white_percent\",\n",
    "    \"black_percent\", \"asian_percent\", \"two_plus_percent\", \"hispanic_percent\",\n",
    "    \"foreign_born_percent\", \"max_avg_high\", \"max_avg_low\", \"mean_diff\"\n",
    "]\n",
    "normalized_data[metrics] = scaler.fit_transform(city_data[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>CensusCode</th>\n",
       "      <th>City_State</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Population</th>\n",
       "      <th>sameHouse1yr</th>\n",
       "      <th>car_percent</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_pp</th>\n",
       "      <th>nightlife_pp</th>\n",
       "      <th>museums_pp</th>\n",
       "      <th>max_avg_high</th>\n",
       "      <th>max_avg_low</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>international_airport</th>\n",
       "      <th>amtrak</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>pop_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>7786</td>\n",
       "      <td>Birmingham, Alabama</td>\n",
       "      <td>33.520682</td>\n",
       "      <td>-86.802433</td>\n",
       "      <td>778756</td>\n",
       "      <td>669858</td>\n",
       "      <td>0.767944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>98.6</td>\n",
       "      <td>13.4</td>\n",
       "      <td>85.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59.78</td>\n",
       "      <td>1365.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>40780</td>\n",
       "      <td>Huntsville, Alabama</td>\n",
       "      <td>34.729847</td>\n",
       "      <td>-86.585901</td>\n",
       "      <td>359389</td>\n",
       "      <td>301448</td>\n",
       "      <td>0.757183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>99.2</td>\n",
       "      <td>11.7</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1006.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>2305</td>\n",
       "      <td>Anchorage, Alaska</td>\n",
       "      <td>61.216313</td>\n",
       "      <td>-149.894852</td>\n",
       "      <td>245624</td>\n",
       "      <td>197589</td>\n",
       "      <td>0.714008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>77.7</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>90.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.97</td>\n",
       "      <td>170.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>69184</td>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "      <td>33.448437</td>\n",
       "      <td>-112.074141</td>\n",
       "      <td>4065338</td>\n",
       "      <td>3457480</td>\n",
       "      <td>0.652450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>115.7</td>\n",
       "      <td>33.8</td>\n",
       "      <td>81.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.30</td>\n",
       "      <td>3102.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tucson</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>88732</td>\n",
       "      <td>Tucson, Arizona</td>\n",
       "      <td>32.222876</td>\n",
       "      <td>-110.974847</td>\n",
       "      <td>879871</td>\n",
       "      <td>728456</td>\n",
       "      <td>0.700604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>110.7</td>\n",
       "      <td>25.6</td>\n",
       "      <td>85.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41.56</td>\n",
       "      <td>2251.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Spokane</td>\n",
       "      <td>Washington</td>\n",
       "      <td>WA</td>\n",
       "      <td>83764</td>\n",
       "      <td>Spokane, Washington</td>\n",
       "      <td>47.657193</td>\n",
       "      <td>-117.423510</td>\n",
       "      <td>458988</td>\n",
       "      <td>386753</td>\n",
       "      <td>0.701422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>99.1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>102.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.84</td>\n",
       "      <td>3300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Charleston</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>WV</td>\n",
       "      <td>15481</td>\n",
       "      <td>Charleston, West Virginia</td>\n",
       "      <td>38.350600</td>\n",
       "      <td>-81.633281</td>\n",
       "      <td>133045</td>\n",
       "      <td>121105</td>\n",
       "      <td>0.760975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>95.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.07</td>\n",
       "      <td>1491.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>WI</td>\n",
       "      <td>53200</td>\n",
       "      <td>Madison, Wisconsin</td>\n",
       "      <td>43.074761</td>\n",
       "      <td>-89.383761</td>\n",
       "      <td>460255</td>\n",
       "      <td>360680</td>\n",
       "      <td>0.626675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>94.1</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.66</td>\n",
       "      <td>3391.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>WI</td>\n",
       "      <td>57466</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>43.038648</td>\n",
       "      <td>-87.909075</td>\n",
       "      <td>1290221</td>\n",
       "      <td>1129593</td>\n",
       "      <td>0.713306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-7.9</td>\n",
       "      <td>102.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51.06</td>\n",
       "      <td>6000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>16237</td>\n",
       "      <td>Cheyenne, Wyoming</td>\n",
       "      <td>41.139981</td>\n",
       "      <td>-104.820246</td>\n",
       "      <td>81378</td>\n",
       "      <td>69092</td>\n",
       "      <td>0.794392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>95.2</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.57</td>\n",
       "      <td>1991.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          City          State StateCode  CensusCode  \\\n",
       "0   Birmingham        Alabama        AL        7786   \n",
       "1   Huntsville        Alabama        AL       40780   \n",
       "2    Anchorage         Alaska        AK        2305   \n",
       "3      Phoenix        Arizona        AZ       69184   \n",
       "4       Tucson        Arizona        AZ       88732   \n",
       "..         ...            ...       ...         ...   \n",
       "95     Spokane     Washington        WA       83764   \n",
       "96  Charleston  West Virginia        WV       15481   \n",
       "97     Madison      Wisconsin        WI       53200   \n",
       "98   Milwaukee      Wisconsin        WI       57466   \n",
       "99    Cheyenne        Wyoming        WY       16237   \n",
       "\n",
       "                   City_State   Latitude   Longitude  Population  \\\n",
       "0         Birmingham, Alabama  33.520682  -86.802433      778756   \n",
       "1         Huntsville, Alabama  34.729847  -86.585901      359389   \n",
       "2           Anchorage, Alaska  61.216313 -149.894852      245624   \n",
       "3            Phoenix, Arizona  33.448437 -112.074141     4065338   \n",
       "4             Tucson, Arizona  32.222876 -110.974847      879871   \n",
       "..                        ...        ...         ...         ...   \n",
       "95        Spokane, Washington  47.657193 -117.423510      458988   \n",
       "96  Charleston, West Virginia  38.350600  -81.633281      133045   \n",
       "97         Madison, Wisconsin  43.074761  -89.383761      460255   \n",
       "98       Milwaukee, Wisconsin  43.038648  -87.909075     1290221   \n",
       "99          Cheyenne, Wyoming  41.139981 -104.820246       81378   \n",
       "\n",
       "    sameHouse1yr  car_percent  ...  activities_pp  nightlife_pp  museums_pp  \\\n",
       "0         669858     0.767944  ...       0.000841      0.000343    0.000048   \n",
       "1         301448     0.757183  ...       0.001049      0.000373    0.000067   \n",
       "2         197589     0.714008  ...       0.003074      0.000464    0.000155   \n",
       "3        3457480     0.652450  ...       0.000467      0.000118    0.000020   \n",
       "4         728456     0.700604  ...       0.001250      0.000295    0.000075   \n",
       "..           ...          ...  ...            ...           ...         ...   \n",
       "95        386753     0.701422  ...       0.001113      0.000423    0.000026   \n",
       "96        121105     0.760975  ...       0.001428      0.000458    0.000113   \n",
       "97        360680     0.626675  ...       0.002042      0.000639    0.000052   \n",
       "98       1129593     0.713306  ...       0.000853      0.000526    0.000061   \n",
       "99         69092     0.794392  ...       0.001954      0.000479    0.000147   \n",
       "\n",
       "    max_avg_high  max_avg_low  mean_diff  international_airport  amtrak  \\\n",
       "0           98.6         13.4       85.2                      1       1   \n",
       "1           99.2         11.7       87.5                      1       1   \n",
       "2           77.7        -13.2       90.9                      1       0   \n",
       "3          115.7         33.8       81.9                      1       1   \n",
       "4          110.7         25.6       85.1                      1       1   \n",
       "..           ...          ...        ...                    ...     ...   \n",
       "95          99.1         -3.0      102.1                      1       1   \n",
       "96          95.3          2.3       93.0                      0       1   \n",
       "97          94.1        -13.9      108.0                      0       1   \n",
       "98          95.0         -7.9      102.9                      1       1   \n",
       "99          95.2        -13.0      108.2                      0       1   \n",
       "\n",
       "    crime_rate  pop_density  \n",
       "0        59.78      1365.37  \n",
       "1        18.00      1006.00  \n",
       "2        38.97       170.60  \n",
       "3        37.30      3102.92  \n",
       "4        41.56      2251.44  \n",
       "..         ...          ...  \n",
       "95       63.84      3300.00  \n",
       "96       23.07      1491.83  \n",
       "97       27.66      3391.00  \n",
       "98       51.06      6000.00  \n",
       "99       40.57      1991.23  \n",
       "\n",
       "[100 rows x 62 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'city_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(city_data[metrics]\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'city_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(city_data[metrics].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>CensusCode</th>\n",
       "      <th>City_State</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Population</th>\n",
       "      <th>sameHouse1yr</th>\n",
       "      <th>car_percent</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_pp</th>\n",
       "      <th>nightlife_pp</th>\n",
       "      <th>museums_pp</th>\n",
       "      <th>max_avg_high</th>\n",
       "      <th>max_avg_low</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>international_airport</th>\n",
       "      <th>amtrak</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>pop_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>7786</td>\n",
       "      <td>Birmingham, Alabama</td>\n",
       "      <td>33.520682</td>\n",
       "      <td>-86.802433</td>\n",
       "      <td>778756</td>\n",
       "      <td>669858</td>\n",
       "      <td>0.900089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103391</td>\n",
       "      <td>0.245643</td>\n",
       "      <td>0.022936</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.459880</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543745</td>\n",
       "      <td>0.041012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>40780</td>\n",
       "      <td>Huntsville, Alabama</td>\n",
       "      <td>34.729847</td>\n",
       "      <td>-86.585901</td>\n",
       "      <td>359389</td>\n",
       "      <td>301448</td>\n",
       "      <td>0.870298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129811</td>\n",
       "      <td>0.270539</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.439521</td>\n",
       "      <td>0.619863</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040251</td>\n",
       "      <td>0.028676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>2305</td>\n",
       "      <td>Anchorage, Alaska</td>\n",
       "      <td>61.216313</td>\n",
       "      <td>-149.894852</td>\n",
       "      <td>245624</td>\n",
       "      <td>197589</td>\n",
       "      <td>0.750773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387019</td>\n",
       "      <td>0.346058</td>\n",
       "      <td>0.093054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141317</td>\n",
       "      <td>0.658676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292962</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>69184</td>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "      <td>33.448437</td>\n",
       "      <td>-112.074141</td>\n",
       "      <td>4065338</td>\n",
       "      <td>3457480</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055887</td>\n",
       "      <td>0.058921</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704192</td>\n",
       "      <td>0.555936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272837</td>\n",
       "      <td>0.100656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tucson</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>88732</td>\n",
       "      <td>Tucson, Arizona</td>\n",
       "      <td>32.222876</td>\n",
       "      <td>-110.974847</td>\n",
       "      <td>879871</td>\n",
       "      <td>728456</td>\n",
       "      <td>0.713666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155341</td>\n",
       "      <td>0.205809</td>\n",
       "      <td>0.040629</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.605988</td>\n",
       "      <td>0.592466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324174</td>\n",
       "      <td>0.071428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Spokane</td>\n",
       "      <td>Washington</td>\n",
       "      <td>WA</td>\n",
       "      <td>83764</td>\n",
       "      <td>Spokane, Washington</td>\n",
       "      <td>47.657193</td>\n",
       "      <td>-117.423510</td>\n",
       "      <td>458988</td>\n",
       "      <td>386753</td>\n",
       "      <td>0.715930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137940</td>\n",
       "      <td>0.312033</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.263473</td>\n",
       "      <td>0.786530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.592673</td>\n",
       "      <td>0.107421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Charleston</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>WV</td>\n",
       "      <td>15481</td>\n",
       "      <td>Charleston, West Virginia</td>\n",
       "      <td>38.350600</td>\n",
       "      <td>-81.633281</td>\n",
       "      <td>133045</td>\n",
       "      <td>121105</td>\n",
       "      <td>0.880796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177950</td>\n",
       "      <td>0.341079</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.326946</td>\n",
       "      <td>0.682648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101350</td>\n",
       "      <td>0.045353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>WI</td>\n",
       "      <td>53200</td>\n",
       "      <td>Madison, Wisconsin</td>\n",
       "      <td>43.074761</td>\n",
       "      <td>-89.383761</td>\n",
       "      <td>460255</td>\n",
       "      <td>360680</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255938</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.132934</td>\n",
       "      <td>0.853881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.110545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>WI</td>\n",
       "      <td>57466</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>43.038648</td>\n",
       "      <td>-87.909075</td>\n",
       "      <td>1290221</td>\n",
       "      <td>1129593</td>\n",
       "      <td>0.748830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104916</td>\n",
       "      <td>0.397510</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.455263</td>\n",
       "      <td>0.204790</td>\n",
       "      <td>0.795662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.438660</td>\n",
       "      <td>0.200102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>16237</td>\n",
       "      <td>Cheyenne, Wyoming</td>\n",
       "      <td>41.139981</td>\n",
       "      <td>-104.820246</td>\n",
       "      <td>81378</td>\n",
       "      <td>69092</td>\n",
       "      <td>0.973307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244761</td>\n",
       "      <td>0.358506</td>\n",
       "      <td>0.087811</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>0.143713</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.312244</td>\n",
       "      <td>0.062496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          City          State StateCode  CensusCode  \\\n",
       "0   Birmingham        Alabama        AL        7786   \n",
       "1   Huntsville        Alabama        AL       40780   \n",
       "2    Anchorage         Alaska        AK        2305   \n",
       "3      Phoenix        Arizona        AZ       69184   \n",
       "4       Tucson        Arizona        AZ       88732   \n",
       "..         ...            ...       ...         ...   \n",
       "95     Spokane     Washington        WA       83764   \n",
       "96  Charleston  West Virginia        WV       15481   \n",
       "97     Madison      Wisconsin        WI       53200   \n",
       "98   Milwaukee      Wisconsin        WI       57466   \n",
       "99    Cheyenne        Wyoming        WY       16237   \n",
       "\n",
       "                   City_State   Latitude   Longitude  Population  \\\n",
       "0         Birmingham, Alabama  33.520682  -86.802433      778756   \n",
       "1         Huntsville, Alabama  34.729847  -86.585901      359389   \n",
       "2           Anchorage, Alaska  61.216313 -149.894852      245624   \n",
       "3            Phoenix, Arizona  33.448437 -112.074141     4065338   \n",
       "4             Tucson, Arizona  32.222876 -110.974847      879871   \n",
       "..                        ...        ...         ...         ...   \n",
       "95        Spokane, Washington  47.657193 -117.423510      458988   \n",
       "96  Charleston, West Virginia  38.350600  -81.633281      133045   \n",
       "97         Madison, Wisconsin  43.074761  -89.383761      460255   \n",
       "98       Milwaukee, Wisconsin  43.038648  -87.909075     1290221   \n",
       "99          Cheyenne, Wyoming  41.139981 -104.820246       81378   \n",
       "\n",
       "    sameHouse1yr  car_percent  ...  activities_pp  nightlife_pp  museums_pp  \\\n",
       "0         669858     0.900089  ...       0.103391      0.245643    0.022936   \n",
       "1         301448     0.870298  ...       0.129811      0.270539    0.035387   \n",
       "2         197589     0.750773  ...       0.387019      0.346058    0.093054   \n",
       "3        3457480     0.580357  ...       0.055887      0.058921    0.004587   \n",
       "4         728456     0.713666  ...       0.155341      0.205809    0.040629   \n",
       "..           ...          ...  ...            ...           ...         ...   \n",
       "95        386753     0.715930  ...       0.137940      0.312033    0.008519   \n",
       "96        121105     0.880796  ...       0.177950      0.341079    0.065531   \n",
       "97        360680     0.509001  ...       0.255938      0.491286    0.025557   \n",
       "98       1129593     0.748830  ...       0.104916      0.397510    0.031455   \n",
       "99         69092     0.973307  ...       0.244761      0.358506    0.087811   \n",
       "\n",
       "    max_avg_high  max_avg_low  mean_diff  international_airport  amtrak  \\\n",
       "0       0.550000     0.459880   0.593607                    1.0     1.0   \n",
       "1       0.565789     0.439521   0.619863                    1.0     1.0   \n",
       "2       0.000000     0.141317   0.658676                    1.0     0.0   \n",
       "3       1.000000     0.704192   0.555936                    1.0     1.0   \n",
       "4       0.868421     0.605988   0.592466                    1.0     1.0   \n",
       "..           ...          ...        ...                    ...     ...   \n",
       "95      0.563158     0.263473   0.786530                    1.0     1.0   \n",
       "96      0.463158     0.326946   0.682648                    0.0     1.0   \n",
       "97      0.431579     0.132934   0.853881                    0.0     1.0   \n",
       "98      0.455263     0.204790   0.795662                    1.0     1.0   \n",
       "99      0.460526     0.143713   0.856164                    0.0     1.0   \n",
       "\n",
       "    crime_rate  pop_density  \n",
       "0     0.543745     0.041012  \n",
       "1     0.040251     0.028676  \n",
       "2     0.292962     0.000000  \n",
       "3     0.272837     0.100656  \n",
       "4     0.324174     0.071428  \n",
       "..         ...          ...  \n",
       "95    0.592673     0.107421  \n",
       "96    0.101350     0.045353  \n",
       "97    0.156664     0.110545  \n",
       "98    0.438660     0.200102  \n",
       "99    0.312244     0.062496  \n",
       "\n",
       "[100 rows x 62 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['restaurants'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, weight \u001b[38;5;129;01min\u001b[39;00m user_weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     44\u001b[0m     cols \u001b[38;5;241m=\u001b[39m variable_to_columns[var]\n\u001b[0;32m---> 45\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m row[cols]\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Use mean if multiple columns per variable\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m metric_score\n\u001b[1;32m     47\u001b[0m city_scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['restaurants'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "user_weights = {\n",
    "    \"COL\": 0.8,  # Example weight from user slider\n",
    "    \"crime\": 0.2,\n",
    "    \"nature\": 0.8,\n",
    "    \"urban\": 0.6,\n",
    "    \"diverse\": 0.4,\n",
    "    \"dining\": 0.6,\n",
    "    \"nightlife\": 1.0,\n",
    "    \"airports\": 0.2,\n",
    "    \"transportation\": 0.8,\n",
    "    \"activities\": 0.2,\n",
    "    \"politics\": 1.0,\n",
    "    \"alignment\": 0.8,\n",
    "    \"climate\": 0.0,\n",
    "    \"seasonal\": 0.4,\n",
    "    \"job-industry\": .5\n",
    "}\n",
    "\n",
    "# Weight association\n",
    "variable_to_columns = {\n",
    "    \"COL\": [\"medianValue\", \"medianRent\"],\n",
    "    \"crime\": [\"crime_rate\"],\n",
    "    \"nature\": [\"parks_pp\"],\n",
    "    \"urban\": [\"pop_density\"],\n",
    "    \"diverse\": [\"white_percent\",\"black_percent\", \"asian_percent\",\"two_plus_percent\", \"hispanic_percent\",\"foreign_born_percent\"],\n",
    "    \"dining\": [\"restaurants\"],\n",
    "    \"nightlife\": [\"nightlife_pp\",\"bars_pp\"],\n",
    "    \"airports\": [\"international_airport\",\"amtrak\"],\n",
    "    \"transportation\": 0.8,\n",
    "    \"activities\": 0.2,\n",
    "    \"politics\": 1.0,\n",
    "    \"alignment\": 0.8,\n",
    "    \"climate\": 0.0,\n",
    "    \"seasonal\": 0.4,\n",
    "    \"job-industry\": .5\n",
    "    # Add all mappings here\n",
    "}\n",
    "\n",
    "# Calculate scores\n",
    "city_scores = []\n",
    "for _, row in normalized_data.iterrows():\n",
    "    score = 0\n",
    "    for var, weight in user_weights.items():\n",
    "        cols = variable_to_columns[var]\n",
    "        metric_score = row[cols].mean()  # Use mean if multiple columns per variable\n",
    "        score += weight * metric_score\n",
    "    city_scores.append(score)\n",
    "\n",
    "# Add scores to the dataset\n",
    "normalized_data[\"score\"] = city_scores\n",
    "\n",
    "# Recommend top cities\n",
    "top_cities = normalized_data.sort_values(\"score\", ascending=False).head(10)\n",
    "print(top_cities[[\"City\", \"score\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State      score\n",
      "83         Memphis, Tennessee  10.451961\n",
      "59  Atlantic City, New Jersey   9.888432\n",
      "52        St. Louis, Missouri   9.490676\n",
      "40     New Orleans, Louisiana   9.337979\n",
      "5       Little Rock, Arkansas   8.997274\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize columns\n",
    "columns_to_normalize = ['crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "                        'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "                        'transit', 'car_percent', 'activities', 'republican', \n",
    "                        'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "                        'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "                        'sales_percent', 'transpo_percent', 'info_percent', \n",
    "                        'finance_percent', 'sci_percent', 'edu_percent', \n",
    "                        'arts_percent', 'other_percent', 'PA_percent']\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 3000,\n",
    "    'crime': 5,\n",
    "    'nature': 1,\n",
    "    'urban': 1,\n",
    "    'diverse': 1,\n",
    "    'dining': 1,\n",
    "    'nightlife': 1,\n",
    "    'airports': 1,\n",
    "    'transportation': 1,\n",
    "    'activities': 1,\n",
    "    'politics': 1,\n",
    "    'alignment': 1,\n",
    "    'climate': 1,\n",
    "    'seasonal': 1,\n",
    "    'job_industry': 'education'\n",
    "}\n",
    "\n",
    "# Apply mandatory filters\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Calculate compatibility scores\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'],\n",
    "    'foreign_born_percent': quiz_inputs['diverse'],\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'max_avg_high': quiz_inputs['climate'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "for industry in columns_to_normalize[-12:]:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Compute scores\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights.keys()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                City_State      score\n",
      "83      Memphis, Tennessee  16.451961\n",
      "40  New Orleans, Louisiana  16.221566\n",
      "52     St. Louis, Missouri  15.744713\n",
      "5    Little Rock, Arkansas  15.613326\n",
      "90    Salt Lake City, Utah  15.395386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize columns\n",
    "columns_to_normalize = ['crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "                        'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "                        'transit', 'car_percent', 'activities', 'republican', \n",
    "                        'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "                        'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "                        'sales_percent', 'transpo_percent', 'info_percent', \n",
    "                        'finance_percent', 'sci_percent', 'edu_percent', \n",
    "                        'arts_percent', 'other_percent', 'PA_percent']\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 3000,\n",
    "    'crime': 3,  # Assuming 5 means a preference for low crime\n",
    "    'nature': 1,\n",
    "    'urban': 1,\n",
    "    'diverse': 1,\n",
    "    'dining': 1,\n",
    "    'nightlife': 1,\n",
    "    'airports': 5,\n",
    "    'transportation': 1,\n",
    "    'activities': 1,\n",
    "    'politics': 1,\n",
    "    'alignment': 5,\n",
    "    'climate': 1,\n",
    "    'seasonal': 1,\n",
    "    'job_industry': 'education'\n",
    "}\n",
    "\n",
    "# Apply mandatory filters\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Calculate compatibility scores\n",
    "weights = {\n",
    "    'crime_rate': 6 - quiz_inputs['crime'],  # Invert the crime preference, higher crime lowers the score\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'],\n",
    "    'foreign_born_percent': quiz_inputs['diverse'],\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'max_avg_high': quiz_inputs['climate'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Add job industry preference\n",
    "for industry in columns_to_normalize[-12:]:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Compute scores\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights.keys()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   crime_rate  parks_pp  pop_density  nonwhite_percent  foreign_born_percent  \\\n",
      "0    0.543745  0.045455     0.041012          0.495763              0.085158   \n",
      "1    0.040251  0.045455     0.028676          0.368644              0.077859   \n",
      "2    0.292962  0.227273     0.000000          0.471751              0.236010   \n",
      "3    0.272837  0.000000     0.100656          0.371469              0.296837   \n",
      "4    0.324174  0.045455     0.071428          0.422316              0.272506   \n",
      "\n",
      "   restaurants  nightlife  transit  car_percent  activities  ...  \\\n",
      "0     0.324324   0.250000      1.0     0.900089    0.088608  ...   \n",
      "1     0.459459   0.250000      1.0     0.870298    0.113924  ...   \n",
      "2     0.756757   0.333333      0.5     0.750773    0.379747  ...   \n",
      "3     0.135135   0.041667      1.0     0.580357    0.037975  ...   \n",
      "4     0.432432   0.208333      1.0     0.713666    0.139241  ...   \n",
      "\n",
      "   manufacturing_percent  sales_percent  transpo_percent  info_percent  \\\n",
      "0               0.352941       0.682353         0.457364      0.227273   \n",
      "1               0.741176       0.270588         0.093023      0.151515   \n",
      "2               0.011765       0.600000         0.620155      0.333333   \n",
      "3               0.329412       0.658824         0.317829      0.272727   \n",
      "4               0.305882       0.611765         0.333333      0.166667   \n",
      "\n",
      "   finance_percent  sci_percent  edu_percent  arts_percent  other_percent  \\\n",
      "0         0.500000     0.271676     0.260989      0.118012       0.717949   \n",
      "1         0.111111     0.942197     0.060440      0.049689       0.692308   \n",
      "2         0.000000     0.300578     0.310440      0.111801       0.410256   \n",
      "3         0.712963     0.462428     0.140110      0.124224       0.538462   \n",
      "4         0.203704     0.393064     0.277473      0.248447       0.641026   \n",
      "\n",
      "   PA_percent  \n",
      "0    0.155556  \n",
      "1    0.466667  \n",
      "2    0.600000  \n",
      "3    0.125926  \n",
      "4    0.222222  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[columns_to_normalize].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State      score  max_avg_low\n",
      "40      New Orleans, Louisiana  25.414225     0.629940\n",
      "2            Anchorage, Alaska  24.556103     0.141317\n",
      "83          Memphis, Tennessee  23.492377     0.462275\n",
      "66  Greensboro, North Carolina  23.434235     0.440719\n",
      "53            Bozeman, Montana  23.166920     0.075449\n",
      "68         Fargo, North Dakota  23.061307     0.000000\n",
      "21              Miami, Florida  22.566598     0.808383\n",
      "87              Houston, Texas  22.482705     0.610778\n",
      "84        Nashville, Tennessee  22.415394     0.407186\n",
      "90        Salt Lake City, Utah  22.377743     0.310180\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data[['City_State', 'score', 'max_avg_low']].sort_values('score', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State      score\n",
      "40      New Orleans, Louisiana  25.426490\n",
      "83          Memphis, Tennessee  22.277708\n",
      "66  Greensboro, North Carolina  21.973732\n",
      "25    West Palm Beach, Florida  21.912109\n",
      "39        Lafayette, Louisiana  21.908497\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize columns\n",
    "columns_to_normalize = ['crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "                        'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "                        'transit', 'car_percent', 'activities', 'republican', \n",
    "                        'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "                        'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "                        'sales_percent', 'transpo_percent', 'info_percent', \n",
    "                        'finance_percent', 'sci_percent', 'edu_percent', \n",
    "                        'arts_percent', 'other_percent', 'PA_percent']\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 3000,\n",
    "    'crime': 4,\n",
    "    'nature': 5,\n",
    "    'urban': 3,\n",
    "    'diverse': 5,\n",
    "    'dining': 4,\n",
    "    'nightlife': 3,\n",
    "    'airports': 2,\n",
    "    'transportation': 1,\n",
    "    'activities': 4,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 1,\n",
    "    'seasonal': 5,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Mandatory filters\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Assign weights\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'max_avg_high': quiz_inputs['climate'],\n",
    "    'max_avg_low': 5 - quiz_inputs['climate'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Add job industry weight\n",
    "for industry in columns_to_normalize[-12:]:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Calculate scores\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights if col in row),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State      score\n",
      "40      New Orleans, Louisiana  24.574042\n",
      "2            Anchorage, Alaska  23.238752\n",
      "83          Memphis, Tennessee  22.280048\n",
      "66  Greensboro, North Carolina  22.237887\n",
      "21              Miami, Florida  22.107694\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 3000,\n",
    "    'crime': 4,\n",
    "    'nature': 5,\n",
    "    'urban': 3,\n",
    "    'diverse': 5,\n",
    "    'dining': 4,\n",
    "    'nightlife': 3,\n",
    "    'airports': 2,\n",
    "    'transportation': 1,\n",
    "    'activities': 4,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 1,  # Prefer colder climates\n",
    "    'seasonal': 1,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply mandatory filters\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Calculate weather score based on climate preference\n",
    "def calculate_weather_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_low']) * 5\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5\n",
    "    else:  # Moderate preference\n",
    "        return ((row['max_avg_high'] + (1 - row['max_avg_low'])) / 2) * 3\n",
    "\n",
    "# Assign weights for other preferences\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'],\n",
    "    'foreign_born_percent': quiz_inputs['diverse'],\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Add industry weight dynamically\n",
    "for industry in columns_to_normalize[-12:]:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Compute weather score for filtered data\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_weather_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute overall compatibility score\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights.keys()) + row['weather_score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State      score  max_avg_high  max_avg_low\n",
      "40      New Orleans, Louisiana  25.414225      0.523684     0.629940\n",
      "2            Anchorage, Alaska  24.556103      0.000000     0.141317\n",
      "83          Memphis, Tennessee  23.492377      0.584211     0.462275\n",
      "66  Greensboro, North Carolina  23.434235      0.518421     0.440719\n",
      "53            Bozeman, Montana  23.166920      0.450000     0.075449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 3000,\n",
    "    'crime': 4,\n",
    "    'nature': 5,\n",
    "    'urban': 3,\n",
    "    'diverse': 5,\n",
    "    'dining': 4,\n",
    "    'nightlife': 3,\n",
    "    'airports': 2,\n",
    "    'transportation': 1,\n",
    "    'activities': 4,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 1,  # Prefer colder climates\n",
    "    'seasonal': 3,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply mandatory filters\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Calculate weather score based on climate preference\n",
    "def calculate_weather_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        # Use inverted normalized max_avg_low\n",
    "        return (1 - row['max_avg_low']) * 5\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        # Use normalized max_avg_high\n",
    "        return row['max_avg_high'] * 5\n",
    "    else:  # Moderate preference\n",
    "        # Blend max_avg_high and inverted max_avg_low for balance\n",
    "        return ((row['max_avg_high'] + (1 - row['max_avg_low'])) / 2) * 3\n",
    "\n",
    "# Assign weights for other preferences\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'],\n",
    "    'foreign_born_percent': quiz_inputs['diverse'],\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Add industry weight dynamically\n",
    "for industry in columns_to_normalize[-12:]:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Compute weather score for filtered data\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_weather_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute overall compatibility score\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights.keys()) + row['weather_score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score', 'max_avg_high', 'max_avg_low']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State      score  max_avg_high  max_avg_low  \\\n",
      "40      New Orleans, Louisiana  24.849074      0.523684     0.629940   \n",
      "2            Anchorage, Alaska  24.410185      0.000000     0.141317   \n",
      "83          Memphis, Tennessee  22.750361      0.584211     0.462275   \n",
      "66  Greensboro, North Carolina  22.196120      0.518421     0.440719   \n",
      "25    West Palm Beach, Florida  21.804460      0.465789     0.750898   \n",
      "\n",
      "    medianRent  \n",
      "40        1160  \n",
      "2         1473  \n",
      "83        1223  \n",
      "66        1202  \n",
      "25        1484  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3317587667.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3317587667.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['score'] = filtered_data.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 5,\n",
    "    'nature': 4,\n",
    "    'urban': 3,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 1,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL']]\n",
    "\n",
    "# Compute climate score\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5\n",
    "    else:  # Moderate preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Calculate weather and compatibility scores\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: sum(weights[col] * row[col] for col in weights.keys()) + row['weather_score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get top recommended cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score', 'max_avg_high', 'max_avg_low', 'medianRent']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State         score  debug_weather_score  \\\n",
      "62    New York City, New York  88707.083913                485.0   \n",
      "9   San Francisco, California  56701.482933                470.0   \n",
      "43      Boston, Massachusetts  42685.845887                482.0   \n",
      "21             Miami, Florida  37505.883511                479.0   \n",
      "32          Chicago, Illinois  37030.429531                485.5   \n",
      "\n",
      "    max_avg_high  max_avg_low  medianRent  \n",
      "62          97.0          7.7        1763  \n",
      "9           94.0         38.8        2367  \n",
      "43          96.4          2.6        2017  \n",
      "21          95.8         42.5        1920  \n",
      "32          97.1         -6.5        1405  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3618064980.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3618064980.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3618064980.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_weather_score'] = filtered_data['weather_score']\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/3618064980.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_total_score'] = filtered_data['score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 5,\n",
    "    'nature': 4,\n",
    "    'urban': 3,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200]\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score'] * 0.5  # Reduced weather score weight (0.5 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Debug intermediate scores\n",
    "filtered_data['debug_weather_score'] = filtered_data['weather_score']\n",
    "filtered_data['debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score', 'debug_weather_score', 'max_avg_high', 'max_avg_low', 'medianRent']]\n",
    "print(top_cities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State         score  debug_weather_score  \\\n",
      "62    New York City, New York  88311.830887             0.507895   \n",
      "9   San Francisco, California  56200.030170             0.428947   \n",
      "43      Boston, Massachusetts  42340.208913             0.492105   \n",
      "21             Miami, Florida  37147.342590             0.476316   \n",
      "32          Chicago, Illinois  36639.207163             0.510526   \n",
      "\n",
      "    max_avg_high  max_avg_low  medianRent  \n",
      "62          97.0          7.7        1763  \n",
      "9           94.0         38.8        2367  \n",
      "43          96.4          2.6        2017  \n",
      "21          95.8         42.5        1920  \n",
      "32          97.1         -6.5        1405  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2084211232.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2084211232.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2084211232.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2084211232.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_weather_score'] = filtered_data['weather_score_normalized']\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2084211232.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_total_score'] = filtered_data['score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 1,\n",
    "    'nature': 4,\n",
    "    'urban': 3,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200]\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize weather score to make sure it's on a comparable scale\n",
    "weather_min = filtered_data['weather_score'].min()\n",
    "weather_max = filtered_data['weather_score'].max()\n",
    "filtered_data['weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score_normalized'] * 0.25  # Reduced weather score weight (0.25 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Debug intermediate scores\n",
    "filtered_data['debug_weather_score'] = filtered_data['weather_score_normalized']\n",
    "filtered_data['debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score', 'debug_weather_score', 'max_avg_high', 'max_avg_low', 'medianRent']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    City_State         score  debug_weather_score  \\\n",
      "32           Chicago, Illinois  12519.534628             0.540390   \n",
      "75  Philadelphia, Pennsylvania  12362.461957             0.568245   \n",
      "77    Providence, Rhode Island  10788.294023             0.526462   \n",
      "48      Minneapolis, Minnesota   8495.290366             0.520891   \n",
      "42         Baltimore, Maryland   7659.973964             0.590529   \n",
      "\n",
      "    max_avg_high  max_avg_low  medianRent  \n",
      "32          97.1         -6.5        1405  \n",
      "75          98.1          8.6        1457  \n",
      "77          96.6          2.0        1348  \n",
      "48          96.4        -16.9        1448  \n",
      "42          98.9          6.9        1565  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['score'] = filtered_data.apply(\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_weather_score'] = filtered_data['weather_score_normalized']\n",
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/2375220581.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['debug_total_score'] = filtered_data['score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 1500,\n",
    "    'crime': 1,\n",
    "    'nature': 4,\n",
    "    'urban': 1,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200]\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data['weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize weather score to make sure it's on a comparable scale\n",
    "weather_min = filtered_data['weather_score'].min()\n",
    "weather_max = filtered_data['weather_score'].max()\n",
    "filtered_data['weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score_normalized'] * 0.25  # Reduced weather score weight (0.25 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply penalties for specific cities or attributes to reduce their ranking\n",
    "# For example, applying a penalty for New Orleans:\n",
    "filtered_data['score'] = filtered_data.apply(\n",
    "    lambda row: row['score'] - 10 if row['City_State'] == 'New Orleans' else row['score'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Debug intermediate scores\n",
    "filtered_data['debug_weather_score'] = filtered_data['weather_score_normalized']\n",
    "filtered_data['debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'score', 'debug_weather_score', 'max_avg_high', 'max_avg_low', 'medianRent']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State   Latitude   Longitude         score\n",
      "62    New York City, New York  40.712728  -74.006015  88311.830887\n",
      "9   San Francisco, California  37.779259 -122.419329  56200.030170\n",
      "43      Boston, Massachusetts  42.355433  -71.060511  42340.208913\n",
      "21             Miami, Florida  25.774173  -80.193620  37147.342590\n",
      "32          Chicago, Illinois  41.875562  -87.624421  36639.207163\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 1,\n",
    "    'nature': 4,\n",
    "    'urban': 3,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200].copy()  # Ensure it's a copy\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data.loc[:, 'weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize weather score to make sure it's on a comparable scale\n",
    "weather_min = filtered_data['weather_score'].min()\n",
    "weather_max = filtered_data['weather_score'].max()\n",
    "filtered_data.loc[:, 'weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data.loc[:, 'score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score_normalized'] * 0.25  # Reduced weather score weight (0.25 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "filtered_data.loc[:, 'debug_weather_score'] = filtered_data['weather_score_normalized']\n",
    "filtered_data.loc[:, 'debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities\n",
    "top_cities = filtered_data.nlargest(5, 'score')[['City_State', 'Latitude','Longitude', 'score']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State   Latitude   Longitude  score_percentage\n",
      "62    New York City, New York  40.712728  -74.006015        100.000000\n",
      "9   San Francisco, California  37.779259 -122.419329         62.929695\n",
      "43      Boston, Massachusetts  42.355433  -71.060511         47.295905\n",
      "21             Miami, Florida  25.774173  -80.193620         41.173383\n",
      "32          Chicago, Illinois  41.875562  -87.624421         40.874192\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 1,\n",
    "    'nature': 4,\n",
    "    'urban': 1,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 1,\n",
    "    'politics': 2,\n",
    "    'alignment': 5,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'arts'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200].copy()  # Ensure it's a copy\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data.loc[:, 'weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize weather score to make sure it's on a comparable scale\n",
    "weather_min = filtered_data['weather_score'].min()\n",
    "weather_max = filtered_data['weather_score'].max()\n",
    "filtered_data.loc[:, 'weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data.loc[:, 'score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score_normalized'] * 0.25  # Reduced weather score weight (0.25 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize the final score to a 0-100% range\n",
    "score_min = filtered_data['score'].min()\n",
    "score_max = filtered_data['score'].max()\n",
    "filtered_data.loc[:, 'score_percentage'] = (filtered_data['score'] - score_min) / (score_max - score_min) * 100\n",
    "\n",
    "# Debugging output\n",
    "filtered_data.loc[:, 'debug_weather_score'] = filtered_data['weather_score_normalized']\n",
    "filtered_data.loc[:, 'debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities with percentage score\n",
    "top_cities = filtered_data.nlargest(5, 'score_percentage')[['City_State', 'Latitude','Longitude', 'score_percentage']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          City_State         score\n",
      "62           New York City, New York  29777.903719\n",
      "9          San Francisco, California  19002.350784\n",
      "43             Boston, Massachusetts  14463.383422\n",
      "21                    Miami, Florida  12713.774727\n",
      "32                 Chicago, Illinois  12577.630116\n",
      "75        Philadelphia, Pennsylvania  12422.677189\n",
      "18  Washington, District of Columbia  11752.883883\n",
      "77          Providence, Rhode Island  10862.324263\n",
      "94               Seattle, Washington   9193.628473\n",
      "6            Los Angeles, California   8609.482546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/xsj3rq1d63n6b8dq4rz88gfc0000gn/T/ipykernel_81745/458819321.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1893.4778720080294' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'score'] = score\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('algorithmData.csv')\n",
    "\n",
    "def recommend_cities(user_inputs):\n",
    "    # Extract user inputs\n",
    "    COL = user_inputs['COL']\n",
    "    crime = user_inputs['crime']\n",
    "    nature = user_inputs['nature']\n",
    "    urban = user_inputs['urban']\n",
    "    diverse = user_inputs['diverse']\n",
    "    dining = user_inputs['dining']\n",
    "    nightlife = user_inputs['nightlife']\n",
    "    airports = user_inputs['airports']\n",
    "    transportation = user_inputs['transportation']\n",
    "    activities = user_inputs['activities']\n",
    "    politics = user_inputs['politics']\n",
    "    alignment = user_inputs['alignment']\n",
    "    climate = user_inputs['climate']\n",
    "    seasonal = user_inputs['seasonal']\n",
    "    job_industry = user_inputs['job_industry']\n",
    "\n",
    "    # Initialize a score column for each city\n",
    "    df['score'] = 0\n",
    "\n",
    "    # Apply the scoring logic based on the user's preferences\n",
    "    for index, row in df.iterrows():\n",
    "        score = 0\n",
    "\n",
    "        # Rent criteria (COL <= medianRent)\n",
    "        if row['medianRent'] <= COL:\n",
    "            score += 1\n",
    "        \n",
    "        # Crime (Lower crime_rate for higher scores)\n",
    "        score += (6 - crime) * (1 / row['crime_rate'])\n",
    "\n",
    "        # Nature (More parks_pp for higher scores)\n",
    "        score += nature * row['parks_pp']\n",
    "\n",
    "        # Urban (Higher population density for higher scores)\n",
    "        score += urban * row['pop_density']\n",
    "\n",
    "        # Diversity (Nonwhite and foreign-born percentages)\n",
    "        score += diverse * (row['nonwhite_percent'] + row['foreign_born_percent'])\n",
    "\n",
    "        # Dining (More restaurants_pp for higher scores)\n",
    "        score += dining * row['restaurants']\n",
    "\n",
    "        # Nightlife (More nightlife options for higher scores)\n",
    "        score += nightlife * row['nightlife']\n",
    "\n",
    "        # Airports (More transit options for higher scores)\n",
    "        score += airports * row['transit']\n",
    "\n",
    "        # Transportation (Higher car_percent for higher scores)\n",
    "        score += transportation * row['car_percent']\n",
    "\n",
    "        # Activities (Higher activities score for higher scores)\n",
    "        score += activities * row['activities']\n",
    "\n",
    "        # Politics (Match political alignment)\n",
    "        score += (politics * alignment) * row['republican']\n",
    "\n",
    "        # Climate (Higher max_avg_high for warmer climates)\n",
    "        score += climate * row['max_avg_high']\n",
    "\n",
    "        # Seasonal (More seasonal variation in weather)\n",
    "        score += seasonal * row['mean_diff']\n",
    "\n",
    "        # Job industry (Match industry preference)\n",
    "        industry_column = job_industry + '_percent'\n",
    "        score += row[industry_column] if industry_column in row else 0\n",
    "\n",
    "        # Set the score for the city\n",
    "        df.at[index, 'score'] = score\n",
    "\n",
    "    # Sort the cities by score and recommend the top ones\n",
    "    recommended_cities = df.sort_values(by='score', ascending=False)\n",
    "\n",
    "    return recommended_cities[['City_State', 'score']].head(10)\n",
    "\n",
    "# Example of user inputs (this would come from your website form)\n",
    "user_inputs = {\n",
    "    'COL': 1100,  # Rent max\n",
    "    'crime': 3,  # Crime concern (1 to 5)\n",
    "    'nature': 4,  # Importance of nature (1 to 5)\n",
    "    'urban': 1,  # Preference for urban areas (1 to 5)\n",
    "    'diverse': 5,  # Preference for diversity (1 to 5)\n",
    "    'dining': 4,  # Importance of dining (1 to 5)\n",
    "    'nightlife': 3,  # Importance of nightlife (1 to 5)\n",
    "    'airports': 4,  # Importance of airports (1 to 5)\n",
    "    'transportation': 2,  # Preference for car (1 to 5)\n",
    "    'activities': 5,  # Importance of activities (1 to 5)\n",
    "    'politics': 3,  # Political alignment (1 to 5)\n",
    "    'alignment': 3,  # Political match importance (1 to 5)\n",
    "    'climate': 2,  # Climate preference (1 to 5)\n",
    "    'seasonal': 3,  # Seasonal variation (1 to 5)\n",
    "    'job_industry': 'education',  # Job industry preference\n",
    "}\n",
    "\n",
    "# Get recommended cities based on user inputs\n",
    "recommended_cities = recommend_cities(user_inputs)\n",
    "\n",
    "# Print the recommended cities\n",
    "print(recommended_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   City_State   Latitude   Longitude  score_1_to_5\n",
      "62    New York City, New York  40.712728  -74.006015      5.000000\n",
      "9   San Francisco, California  37.779259 -122.419329      3.518414\n",
      "43      Boston, Massachusetts  42.355433  -71.060511      2.893579\n",
      "21             Miami, Florida  25.774173  -80.193620      2.645012\n",
      "32          Chicago, Illinois  41.875562  -87.624421      2.636917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"algorithmData.csv\")\n",
    "\n",
    "# Normalize relevant columns\n",
    "columns_to_normalize = [\n",
    "    'crime_rate', 'parks_pp', 'pop_density', 'nonwhite_percent', \n",
    "    'foreign_born_percent', 'restaurants', 'nightlife', \n",
    "    'transit', 'car_percent', 'activities', 'republican', \n",
    "    'max_avg_high', 'max_avg_low', 'mean_diff',\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent',\n",
    "    'sales_percent', 'transpo_percent', 'info_percent', \n",
    "    'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent'\n",
    "]\n",
    "\n",
    "# Example quiz inputs\n",
    "quiz_inputs = {\n",
    "    'COL': 2500,\n",
    "    'crime': 1,\n",
    "    'nature': 4,\n",
    "    'urban': 1,\n",
    "    'diverse': 4,\n",
    "    'dining': 4,\n",
    "    'nightlife': 2,\n",
    "    'airports': 3,\n",
    "    'transportation': 1,\n",
    "    'activities': 5,\n",
    "    'politics': 2,\n",
    "    'alignment': 3,\n",
    "    'climate': 5,  # Prefer colder climates\n",
    "    'seasonal': 4,\n",
    "    'job_industry': 'scientific'\n",
    "}\n",
    "\n",
    "# Apply cost filter\n",
    "filtered_data = data[data['medianRent'] <= quiz_inputs['COL'] + 200].copy()  # Ensure it's a copy\n",
    "\n",
    "# Assign weights based on quiz inputs\n",
    "weights = {\n",
    "    'crime_rate': quiz_inputs['crime'],\n",
    "    'parks_pp': quiz_inputs['nature'],\n",
    "    'pop_density': quiz_inputs['urban'],\n",
    "    'nonwhite_percent': quiz_inputs['diverse'] / 2,\n",
    "    'foreign_born_percent': quiz_inputs['diverse'] / 2,\n",
    "    'restaurants': quiz_inputs['dining'],\n",
    "    'nightlife': quiz_inputs['nightlife'],\n",
    "    'transit': quiz_inputs['airports'],\n",
    "    'car_percent': quiz_inputs['transportation'],\n",
    "    'activities': quiz_inputs['activities'],\n",
    "    'republican': quiz_inputs['alignment'] * quiz_inputs['politics'],\n",
    "    'mean_diff': quiz_inputs['seasonal']\n",
    "}\n",
    "\n",
    "# Include job industry preference\n",
    "for industry in [\n",
    "    'NR_percent', 'construction_percent', 'manufacturing_percent', 'sales_percent', \n",
    "    'transpo_percent', 'info_percent', 'finance_percent', 'sci_percent', 'edu_percent', \n",
    "    'arts_percent', 'other_percent', 'PA_percent']:\n",
    "    weights[industry] = 5 if industry == f\"{quiz_inputs['job_industry']}_percent\" else 0\n",
    "\n",
    "# Adjust weather scoring\n",
    "def calculate_climate_score(row, climate):\n",
    "    if climate == 1:  # Prefer colder climates\n",
    "        return (1 - row['max_avg_high']) * 5  # Negative impact of high max temp\n",
    "    elif climate == 5:  # Prefer warmer climates\n",
    "        return row['max_avg_high'] * 5  # Positive impact of high max temp\n",
    "    else:  # Neutral preference\n",
    "        return ((row['max_avg_high'] + row['mean_diff']) / 2) * 3  # Moderate impact\n",
    "\n",
    "# Calculate weather score\n",
    "filtered_data.loc[:, 'weather_score'] = filtered_data.apply(\n",
    "    lambda row: calculate_climate_score(row, quiz_inputs['climate']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Normalize weather score to make sure it's on a comparable scale\n",
    "weather_min = filtered_data['weather_score'].min()\n",
    "weather_max = filtered_data['weather_score'].max()\n",
    "filtered_data.loc[:, 'weather_score_normalized'] = (filtered_data['weather_score'] - weather_min) / (weather_max - weather_min)\n",
    "\n",
    "# Adjust final score by reducing the weight of weather's impact\n",
    "filtered_data.loc[:, 'score'] = filtered_data.apply(\n",
    "    lambda row: (\n",
    "        sum(weights[col] * row[col] for col in weights.keys()) +\n",
    "        row['weather_score_normalized'] * 0.25  # Reduced weather score weight (0.25 is lighter)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Rescale the score to 1-5 range\n",
    "score_min = filtered_data['score'].min()\n",
    "score_max = filtered_data['score'].max()\n",
    "filtered_data.loc[:, 'score_1_to_5'] = 1 + ((filtered_data['score'] - score_min) / (score_max - score_min)) * 4\n",
    "\n",
    "# Debugging output\n",
    "filtered_data.loc[:, 'debug_weather_score'] = filtered_data['weather_score_normalized']\n",
    "filtered_data.loc[:, 'debug_total_score'] = filtered_data['score']\n",
    "\n",
    "# Get top recommended cities with rescaled 1-5 score\n",
    "top_cities = filtered_data.nlargest(5, 'score_1_to_5')[['City_State', 'Latitude', 'Longitude', 'score_1_to_5']]\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def match_user_to_city(user_data, city_data):\n",
    "    # Normalize the user preferences and city data\n",
    "    user_array = np.array([user_data[attribute] for attribute in city_data.columns])\n",
    "    city_array = np.array(city_data.values)\n",
    "    \n",
    "    # Calculate similarity score (e.g., Euclidean distance)\n",
    "    distances = np.linalg.norm(city_array - user_array, axis=1)\n",
    "    \n",
    "    # Return the cities sorted by best match (lowest distance)\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    best_matches = city_data.iloc[sorted_indices]\n",
    "    return best_matches.head(5)  # Top 5 recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'State'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m match_user_to_city(quiz_inputs, data)\n",
      "Cell \u001b[0;32mIn[87], line 6\u001b[0m, in \u001b[0;36mmatch_user_to_city\u001b[0;34m(user_data, city_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m relevant_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m city_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Normalize the user preferences and city data for matching attributes\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m user_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([user_data[attribute] \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m relevant_columns])\n\u001b[1;32m      7\u001b[0m city_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(city_data[relevant_columns]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate similarity score (e.g., Euclidean distance)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'State'"
     ]
    }
   ],
   "source": [
    "match_user_to_city(quiz_inputs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_user_to_city(user_data, city_data):\n",
    "    # Exclude non-matching columns, such as 'City'\n",
    "    relevant_columns = [col for col in city_data.columns if col != \"City\"]\n",
    "    \n",
    "    # Normalize the user preferences and city data for matching attributes\n",
    "    user_array = np.array([user_data[attribute] for attribute in relevant_columns])\n",
    "    city_array = np.array(city_data[relevant_columns].values)\n",
    "    \n",
    "    # Calculate similarity score (e.g., Euclidean distance)\n",
    "    distances = np.linalg.norm(city_array - user_array, axis=1)\n",
    "    \n",
    "    # Return the cities sorted by best match (lowest distance)\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    best_matches = city_data.iloc[sorted_indices]\n",
    "    return best_matches.head(5)  # Top 5 recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               City_State      score  medianRent\n",
      "      Fargo, North Dakota 148.271863         992\n",
      "Sioux Falls, South Dakota 124.620803         986\n",
      "         Bozeman, Montana 120.947856        1852\n",
      "     Rochester, Minnesota 110.512000        1446\n",
      "        Buffalo, New York 102.265345        1066\n",
      "        Anchorage, Alaska 101.278531        1473\n",
      "        Cheyenne, Wyoming  99.508480        1141\n",
      "       Madison, Wisconsin  94.380014        1395\n",
      "   Minneapolis, Minnesota  92.318180        1448\n",
      "         Des Moines, Iowa  90.974628        1178\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('algorithmData1.csv')\n",
    "\n",
    "# Normalize relevant columns\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_normalize = [ 'crime_rate', 'parks_pp', 'pop_density', \n",
    "    'nonwhite_percent', 'foreign_born_percent', 'restaurants', \n",
    "    'nightlife', 'transit', 'car_percent', 'activities', \n",
    "    'republican', 'max_avg_high', 'mean_diff',\n",
    "    'natural-resources_percent', 'construction_percent', 'manufacturing_percent', \n",
    "    'sales_percent', 'transportation_percent', 'information_percent', \n",
    "    'finance_percent', 'scientific_percent', 'education_percent', \n",
    "    'arts_percent', 'other_percent', 'public-administration_percent'\n",
    "]\n",
    "data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
    "\n",
    "# Example user inputs (replace these with dynamic inputs)\n",
    "user_preferences = {\n",
    "    'COL': 3000,  # Cost of living preference\n",
    "    'crime': 4,  # Safety\n",
    "    'nature': 5,  # Parks\n",
    "    'urban': 1,  # Urban/rural\n",
    "    'diverse': 5,  # Diversity\n",
    "    'dining': 4,  # Dining\n",
    "    'nightlife': 4,  # Nightlife\n",
    "    'airports': 3,  # Transit access\n",
    "    'transportation': 2,  # Driving vs public transit\n",
    "    'activities': 5,  # Entertainment\n",
    "    'politics': 5,  # Political preference\n",
    "    'alignment': 5,  # Political alignment importance\n",
    "    'climate': 2,  # Hot vs cold\n",
    "    'seasonal': 3,  # Seasonal changes\n",
    "    'job-industry': 'education'  # Job industry preference\n",
    "}\n",
    "\n",
    "# Match job industry\n",
    "data['job_match'] = data[f\"{user_preferences['job-industry']}_percent\"]\n",
    "\n",
    "data = data[data['medianRent'] <= user_preferences['COL'] + 400]\n",
    "# Weight variables based on user inputs\n",
    "# Calculate weighted score based on user preferences\n",
    "weather = 0\n",
    "if user_preferences['climate'] in [1, 2]:\n",
    "    weather =  (1- data['max_avg_low']) * (6 - user_preferences['climate'] )\n",
    "elif user_preferences['climate'] in [4,5]:\n",
    "    weather = data['max_avg_high'] * user_preferences['climate'] \n",
    "\n",
    "density = 0\n",
    "if user_preferences['urban'] in [1, 2]:\n",
    "    density = (1 - data['pop_density']) * (6 - user_preferences['urban'])  # Prefer lower density\n",
    "elif user_preferences['urban'] in [3, 4, 5]:\n",
    "    density = data['pop_density'] * user_preferences['urban']  # Prefer higher density\n",
    "\n",
    "data['score'] = (\n",
    "    data['crime_rate'] * (6 - user_preferences['crime']) +  # Lower crime rate preferred\n",
    "    data['parks_pp'] * user_preferences['nature'] +        # Nature preference\n",
    "    density +      # Urban/rural preference\n",
    "    (data['nonwhite_percent'] + data['foreign_born_percent']) * user_preferences['diverse'] +\n",
    "    data['restaurants'] * user_preferences['dining'] +\n",
    "    data['nightlife'] * user_preferences['nightlife'] +\n",
    "    data['transit'] * user_preferences['airports'] +\n",
    "    data['car_percent'] * user_preferences['transportation'] +\n",
    "    data['activities'] * user_preferences['activities'] +\n",
    "    data['republican'] * user_preferences['politics'] * user_preferences['alignment'] +\n",
    "    weather +\n",
    "    data['mean_diff'] * user_preferences['seasonal'] +\n",
    "    data['job_match'] * 5  # Job match is always maximally weighted\n",
    ")\n",
    "\n",
    "# Normalize the score to range 0-1 for better comparability\n",
    "\n",
    "\n",
    "# Rank cities by score\n",
    "recommendations = data.sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "# Output recommendations\n",
    "print(recommendations[['City_State', 'score', 'medianRent']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                City_State      score\n",
      "21          Miami, Florida  46.752732\n",
      "85           Austin, Texas  44.748798\n",
      "40  New Orleans, Louisiana  44.550337\n",
      "23    Tallahassee, Florida  44.044434\n",
      "86           Dallas, Texas  43.158885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('algorithmData1.csv')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_normalize = [ 'crime_rate', 'parks_pp', 'pop_density', \n",
    "    'nonwhite_percent', 'foreign_born_percent', 'restaurants', \n",
    "    'nightlife', 'transit', 'car_percent', 'activities', \n",
    "    'republican', 'democrat', 'max_avg_high', 'mean_diff',\n",
    "    'natural-resources_percent', 'construction_percent', 'manufacturing_percent', \n",
    "    'sales_percent', 'transportation_percent', 'information_percent', \n",
    "    'finance_percent', 'scientific_percent', 'education_percent', \n",
    "    'arts_percent', 'other_percent', 'public-administration_percent'\n",
    "]\n",
    "data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
    "\n",
    "# User preferences\n",
    "user_preferences = {\n",
    "    'COL': 3000,  # Cost of living preference\n",
    "    'crime': 1,  # Safety\n",
    "    'nature': 1,  # Parks\n",
    "    'urban': 4,  # Urban/rural\n",
    "    'diverse': 5,  # Diversity\n",
    "    'dining': 4,  # Dining\n",
    "    'nightlife': 4,  # Nightlife\n",
    "    'airports': 1,  # Transit access\n",
    "    'transportation': 2,  # Driving vs public transit\n",
    "    'activities': 1,  # Entertainment\n",
    "    'politics': 5,  # Political preference\n",
    "    'alignment': 5,  # Political alignment importance\n",
    "    'climate': 5,  # Hot vs cold\n",
    "    'seasonal': 1,  # Seasonal changes\n",
    "    'job-industry': 'information'  # Job industry preference\n",
    "}\n",
    "\n",
    "data['score'] = 0\n",
    "\n",
    "data['job_match'] = data[f\"{user_preferences['job-industry']}_percent\"]\n",
    "\n",
    "data = data[data['medianRent'] <= user_preferences['COL'] + 400]\n",
    "\n",
    "weather = 0\n",
    "if user_preferences['climate'] in [1, 2]:\n",
    "    weather = (1 - data['max_avg_low']) * (6 - user_preferences['climate'])\n",
    "elif user_preferences['climate'] in [4, 5]:\n",
    "    weather = data['max_avg_high'] * user_preferences['climate']\n",
    "\n",
    "density = 0\n",
    "if user_preferences['urban'] in [1, 2]:\n",
    "    density = (1 - data['pop_density']) * (6 - user_preferences['urban'])  # Prefer lower density\n",
    "elif user_preferences['urban'] in [4, 5]:\n",
    "    density = data['pop_density'] * user_preferences['urban']  # Prefer higher density\n",
    "\n",
    "politics = 0\n",
    "if user_preferences['politics'] in [1, 2]:\n",
    "    politics = data['democrat'] * (6 - user_preferences['politics'])  # Prefer lower density\n",
    "elif user_preferences['politics'] in [4, 5]:\n",
    "    politics = data['republican'] * user_preferences['politics']  # Prefer higher density\n",
    "\n",
    "data['score'] = (\n",
    "    (1 - data['crime_rate']) * (6 - user_preferences['crime']) +  # Lower crime rate preferred\n",
    "    data['parks_pp'] * user_preferences['nature'] +        # Nature preference\n",
    "    density +                                              # Urban/rural preference\n",
    "    (data['nonwhite_percent'] + data['foreign_born_percent']) * user_preferences['diverse'] +\n",
    "    data['restaurants'] * user_preferences['dining'] +\n",
    "    data['nightlife'] * user_preferences['nightlife'] +\n",
    "    data['transit'] * user_preferences['airports'] +\n",
    "    data['car_percent'] * user_preferences['transportation'] +\n",
    "    data['activities'] * user_preferences['activities'] +\n",
    "    politics * user_preferences['alignment'] +\n",
    "    weather +\n",
    "    data['mean_diff'] * user_preferences['seasonal'] +\n",
    "    data['job_match'] * 5  # Job match is always maximally weighted\n",
    ")\n",
    "\n",
    "recommendations = data.sort_values(by='score', ascending=False).head(5)\n",
    "# Output recommendations\n",
    "print(recommendations[['City_State', 'score']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User preferences\n",
    "user_preferences = {\n",
    "    'COL': 3000,  # Cost of living preference\n",
    "    'crime': 1,  # Safety\n",
    "    'nature': 1,  # Parks\n",
    "    'urban': 4,  # Urban/rural\n",
    "    'diverse': 5,  # Diversity\n",
    "    'dining': 4,  # Dining\n",
    "    'nightlife': 4,  # Nightlife\n",
    "    'airports': 1,  # Transit access\n",
    "    'transportation': 2,  # Driving vs public transit\n",
    "    'activities': 1,  # Entertainment\n",
    "    'politics': 5,  # Political preference\n",
    "    'alignment': 5,  # Political alignment importance\n",
    "    'climate': 5,  # Hot vs cold\n",
    "    'seasonal': 1,  # Seasonal changes\n",
    "    'job-industry': 'information'  # Job industry preference\n",
    "}\n",
    "\n",
    "def cityReccomender(user_preferences):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    data = pd.read_csv('dataset.csv')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_normalize = [ 'crime_rate', 'parks_pp', 'pop_density', \n",
    "        'nonwhite_percent', 'foreign_born_percent', 'restaurants', \n",
    "        'nightlife', 'transit', 'car_percent', 'activities', \n",
    "        'republican', 'democrat', 'max_avg_high', 'mean_diff',\n",
    "        'natural-resources_percent', 'construction_percent', 'manufacturing_percent', \n",
    "        'sales_percent', 'transportation_percent', 'information_percent', \n",
    "        'finance_percent', 'scientific_percent', 'education_percent', \n",
    "        'arts_percent', 'other_percent', 'public-administration_percent'\n",
    "    ]\n",
    "    data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
    "\n",
    "\n",
    "\n",
    "    data['score'] = 0\n",
    "\n",
    "    data['job_match'] = data[f\"{user_preferences['job-industry']}_percent\"]\n",
    "\n",
    "    data = data[data['medianRent'] <= user_preferences['COL'] + 400]\n",
    "\n",
    "    weather = 0\n",
    "    if user_preferences['climate'] in [1, 2]:\n",
    "        weather = (1 - data['max_avg_low']) * (6 - user_preferences['climate'])\n",
    "    elif user_preferences['climate'] in [4, 5]:\n",
    "        weather = data['max_avg_high'] * user_preferences['climate']\n",
    "\n",
    "    density = 0\n",
    "    if user_preferences['urban'] in [1, 2]:\n",
    "        density = (1 - data['pop_density']) * (6 - user_preferences['urban'])  # Prefer lower density\n",
    "    elif user_preferences['urban'] in [4, 5]:\n",
    "        density = data['pop_density'] * user_preferences['urban']  # Prefer higher density\n",
    "\n",
    "    politics = 0\n",
    "    if user_preferences['politics'] in [1, 2]:\n",
    "        politics = data['democrat'] * (6 - user_preferences['politics'])  # Prefer lower density\n",
    "    elif user_preferences['politics'] in [4, 5]:\n",
    "        politics = data['republican'] * user_preferences['politics']  # Prefer higher density\n",
    "\n",
    "    data['score'] = (\n",
    "        (1 - data['crime_rate']) * (6 - user_preferences['crime']) +  # Lower crime rate preferred\n",
    "        data['parks_pp'] * user_preferences['nature'] +        # Nature preference\n",
    "        density +                                              # Urban/rural preference\n",
    "        (data['nonwhite_percent'] + data['foreign_born_percent']) * user_preferences['diverse'] +\n",
    "        data['restaurants'] * user_preferences['dining'] +\n",
    "        data['nightlife'] * user_preferences['nightlife'] +\n",
    "        data['transit'] * user_preferences['airports'] +\n",
    "        data['car_percent'] * user_preferences['transportation'] +\n",
    "        data['activities'] * user_preferences['activities'] +\n",
    "        politics * user_preferences['alignment'] +\n",
    "        weather +\n",
    "        data['mean_diff'] * user_preferences['seasonal'] +\n",
    "        data['job_match'] * 5  # Job match is always maximally weighted\n",
    "    )\n",
    "\n",
    "    recommendations = data.sort_values(by='score', ascending=False).head(5)\n",
    "    # Output recommendations\n",
    "    return recommendations[['City_State', 'score', 'Latitude', 'Longitude', 'blurb', 'image', 'bullets']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City_State</th>\n",
       "      <th>score</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>blurb</th>\n",
       "      <th>image</th>\n",
       "      <th>bullets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Miami, Florida</td>\n",
       "      <td>46.752732</td>\n",
       "      <td>25.774173</td>\n",
       "      <td>-80.193620</td>\n",
       "      <td>Miami is a bustling coastal city known for its...</td>\n",
       "      <td>https://blog.viajemos.com/wp-content/uploads/2...</td>\n",
       "      <td>[ \"Beaches and nightlife.\", \"Diverse cultural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Austin, Texas</td>\n",
       "      <td>44.748798</td>\n",
       "      <td>30.271129</td>\n",
       "      <td>-97.743700</td>\n",
       "      <td>Austin is a vibrant city known for its live mu...</td>\n",
       "      <td>https://www.redfin.com/blog/wp-content/uploads...</td>\n",
       "      <td>[\"Live Music Capital of the World.\", \"Thriving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>New Orleans, Louisiana</td>\n",
       "      <td>44.550337</td>\n",
       "      <td>29.975998</td>\n",
       "      <td>-90.078213</td>\n",
       "      <td>New Haven offers a blend of history, academic ...</td>\n",
       "      <td>https://content.r9cdn.net/rimg/dimg/6e/52/9f9e...</td>\n",
       "      <td>[\"Vibrant jazz and food culture.\", \"Historic n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Tallahassee, Florida</td>\n",
       "      <td>44.044434</td>\n",
       "      <td>30.438083</td>\n",
       "      <td>-84.280933</td>\n",
       "      <td>Tallahassee, Florida’s capital, combines South...</td>\n",
       "      <td>https://www.worldatlas.com/upload/7e/e9/b1/shu...</td>\n",
       "      <td>\"Lively college-town vibe.\", \"Affordable livin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>43.158885</td>\n",
       "      <td>32.776272</td>\n",
       "      <td>-96.796856</td>\n",
       "      <td>Dallas is a dynamic, sprawling city with a boo...</td>\n",
       "      <td>https://images.surferseo.art/fc828a2c-43cb-4e1...</td>\n",
       "      <td>[\"Thriving job market.\", \"Cultural diversity.\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                City_State      score   Latitude  Longitude  \\\n",
       "59          Miami, Florida  46.752732  25.774173 -80.193620   \n",
       "6            Austin, Texas  44.748798  30.271129 -97.743700   \n",
       "64  New Orleans, Louisiana  44.550337  29.975998 -90.078213   \n",
       "92    Tallahassee, Florida  44.044434  30.438083 -84.280933   \n",
       "26           Dallas, Texas  43.158885  32.776272 -96.796856   \n",
       "\n",
       "                                                blurb  \\\n",
       "59  Miami is a bustling coastal city known for its...   \n",
       "6   Austin is a vibrant city known for its live mu...   \n",
       "64  New Haven offers a blend of history, academic ...   \n",
       "92  Tallahassee, Florida’s capital, combines South...   \n",
       "26  Dallas is a dynamic, sprawling city with a boo...   \n",
       "\n",
       "                                                image  \\\n",
       "59  https://blog.viajemos.com/wp-content/uploads/2...   \n",
       "6   https://www.redfin.com/blog/wp-content/uploads...   \n",
       "64  https://content.r9cdn.net/rimg/dimg/6e/52/9f9e...   \n",
       "92  https://www.worldatlas.com/upload/7e/e9/b1/shu...   \n",
       "26  https://images.surferseo.art/fc828a2c-43cb-4e1...   \n",
       "\n",
       "                                              bullets  \n",
       "59  [ \"Beaches and nightlife.\", \"Diverse cultural ...  \n",
       "6   [\"Live Music Capital of the World.\", \"Thriving...  \n",
       "64  [\"Vibrant jazz and food culture.\", \"Historic n...  \n",
       "92  \"Lively college-town vibe.\", \"Affordable livin...  \n",
       "26  [\"Thriving job market.\", \"Cultural diversity.\"...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityReccomender(user_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
